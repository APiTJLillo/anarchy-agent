// local_provider.a.i
// Local LLM provider implementation for LLM integration (using Ollama)

// Import dependencies
ğŸ§  provider_interface = require("./provider_interface.a.i")
ğŸ§  http_utils = require("../utils/http_utils.a.i")

ğŸ”  create_local_provider() {
  // Get the base provider interface
  ğŸ§  base = provider_interface.create_provider_interface()
  ğŸ§  http = http_utils.create_http_utils()
  
  // Create the Local provider by extending the base interface
  ğŸ§  local_provider = {
    // Provider metadata
    name: "local",
    description: "Local LLM provider using Ollama",
    version: "1.0.0",
    
    // Initialize the provider with configuration
    initialize: ğŸ”(config) {
      if (!config) {
        ğŸ“¤ { error: "Configuration is required" }
      }
      
      // Create provider instance
      ğŸ§  provider = {
        endpoint: config.endpoint || "http://localhost:11434",
        model: config.model || "llama3",
        temperature: config.temperature !== undefined ? config.temperature : 0.7,
        max_tokens: config.max_tokens || 1000
      }
      
      ğŸ“¤ provider
    },
    
    // Validate that the provider's configuration is correct
    validate_credentials: ğŸ”(provider) {
      if (!provider || !provider.endpoint) {
        ğŸ“¤ false
      }
      
      // Make a simple API call to validate the endpoint
      ğŸ§  response = http.get(provider.endpoint + "/api/tags")
      
      // Check if the response contains an error
      if (response.error) {
        ğŸ“¤ false
      }
      
      ğŸ“¤ true
    },
    
    // Generate a response from the local LLM
    generate: ğŸ”(provider, prompt, options) {
      if (!provider || !provider.endpoint) {
        ğŸ“¤ { error: "Provider not initialized or missing endpoint" }
      }
      
      // Prepare request data
      ğŸ§  data = {
        model: options?.model || provider.model,
        prompt: prompt,
        temperature: options?.temperature !== undefined ? options.temperature : provider.temperature,
        max_tokens: options?.max_tokens || provider.max_tokens,
        stream: false
      }
      
      // Add system message if provided
      if (options?.system_message) {
        data.system = options.system_message
      }
      
      // Make the API request
      ğŸ§  response = http.post(provider.endpoint + "/api/generate", data)
      
      // Check for errors
      if (response.error) {
        ğŸ“¤ { error: "API request failed: " + response.error }
      }
      
      // Extract the response text
      if (response.response) {
        ğŸ“¤ response.response
      }
      
      ğŸ“¤ { error: "Unexpected response format" }
    },
    
    // Stream a response from the local LLM
    stream: ğŸ”(provider, prompt, callback, options) {
      if (!provider || !provider.endpoint) {
        ğŸ“¤ { error: "Provider not initialized or missing endpoint" }
      }
      
      // Prepare request data
      ğŸ§  data = {
        model: options?.model || provider.model,
        prompt: prompt,
        temperature: options?.temperature !== undefined ? options.temperature : provider.temperature,
        max_tokens: options?.max_tokens || provider.max_tokens,
        stream: true
      }
      
      // Add system message if provided
      if (options?.system_message) {
        data.system = options.system_message
      }
      
      // In a real implementation, we would use a streaming approach
      // For now, we'll simulate it with a non-streaming request
      data.stream = false
      
      // Make the API request (non-streaming for now)
      ğŸ§  response = http.post(provider.endpoint + "/api/generate", data)
      
      // Check for errors
      if (response.error) {
        ğŸ“¤ { error: "API request failed: " + response.error }
      }
      
      // Extract the response text and call the callback
      if (response.response) {
        callback(response.response)
        ğŸ“¤ true
      }
      
      ğŸ“¤ { error: "Unexpected response format" }
    },
    
    // Estimate the number of tokens in the text
    get_token_count: ğŸ”(provider, text) {
      // Simple estimation: ~4 characters per token for English text
      ğŸ“¤ Math.ceil(text.length / 4)
    },
    
    // Get the capabilities of this provider
    get_capabilities: ğŸ”(provider) {
      // Capabilities depend on the specific model
      // For now, we'll use conservative defaults
      ğŸ§  capabilities = {
        supports_streaming: true,
        supports_function_calling: false,
        supports_vision: false,
        max_context_length: 4096  // Default, varies by model
      }
      
      // Adjust based on model
      if (provider.model.includes("llama3")) {
        capabilities.max_context_length = 8192
      } else if (provider.model.includes("mistral")) {
        capabilities.max_context_length = 8192
      }
      
      ğŸ“¤ capabilities
    }
  }
  
  // Merge the base interface with our implementation
  for (ğŸ§  key in base) {
    if (!local_provider[key]) {
      local_provider[key] = base[key]
    }
  }
  
  ğŸ“¤ local_provider
}

// Export the Local provider creator function
ğŸ“¤ {
  create_local_provider: create_local_provider
}
