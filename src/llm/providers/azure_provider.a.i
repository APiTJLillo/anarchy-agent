// azure_provider.a.i
// Azure OpenAI provider implementation for LLM integration

// Import dependencies
ğŸ§  provider_interface = require("./provider_interface.a.i")
ğŸ§  http_utils = require("../utils/http_utils.a.i")

ğŸ”  create_azure_provider() {
  // Get the base provider interface
  ğŸ§  base = provider_interface.create_provider_interface()
  ğŸ§  http = http_utils.create_http_utils()
  
  // Create the Azure OpenAI provider by extending the base interface
  ğŸ§  azure_provider = {
    // Provider metadata
    name: "azure_openai",
    description: "Azure OpenAI API provider for GPT models",
    version: "1.0.0",
    
    // Initialize the provider with configuration
    initialize: ğŸ”(config) {
      if (!config) {
        ğŸ“¤ { error: "Configuration is required" }
      }
      
      if (!config.endpoint || !config.deployment_name) {
        ğŸ“¤ { error: "Azure OpenAI requires endpoint and deployment_name" }
      }
      
      // Create provider instance
      ğŸ§  provider = {
        api_key: config.api_key || "",
        endpoint: config.endpoint,
        deployment_name: config.deployment_name,
        api_version: config.api_version || "2023-05-15",
        temperature: config.temperature !== undefined ? config.temperature : 0.7,
        max_tokens: config.max_tokens || 1000
      }
      
      ğŸ“¤ provider
    },
    
    // Validate that the provider's credentials are correct
    validate_credentials: ğŸ”(provider) {
      if (!provider || !provider.api_key || !provider.endpoint || !provider.deployment_name) {
        ğŸ“¤ false
      }
      
      // Make a simple API call to validate credentials
      ğŸ§  headers = {
        "api-key": provider.api_key,
        "Content-Type": "application/json"
      }
      
      // Construct the URL for the deployments endpoint
      ğŸ§  url = provider.endpoint + "/openai/deployments?api-version=" + provider.api_version
      
      ğŸ§  response = http.get(url, headers)
      
      // Check if the response contains an error
      if (response.error || response.statusCode === 401) {
        ğŸ“¤ false
      }
      
      ğŸ“¤ true
    },
    
    // Generate a response from the Azure OpenAI API
    generate: ğŸ”(provider, prompt, options) {
      if (!provider || !provider.api_key) {
        ğŸ“¤ { error: "Provider not initialized or missing API key" }
      }
      
      // Prepare request headers
      ğŸ§  headers = {
        "api-key": provider.api_key,
        "Content-Type": "application/json"
      }
      
      // Prepare request data
      ğŸ§  data = {
        messages: [
          { role: "user", content: prompt }
        ],
        temperature: options?.temperature !== undefined ? options.temperature : provider.temperature,
        max_tokens: options?.max_tokens || provider.max_tokens
      }
      
      // Add system message if provided
      if (options?.system_message) {
        data.messages.unshift({ role: "system", content: options.system_message })
      }
      
      // Construct the URL for the chat completions endpoint
      ğŸ§  url = provider.endpoint + "/openai/deployments/" + provider.deployment_name + "/chat/completions?api-version=" + provider.api_version
      
      // Make the API request
      ğŸ§  response = http.post(url, data, headers)
      
      // Check for errors
      if (response.error) {
        ğŸ“¤ { error: "API request failed: " + response.error }
      }
      
      // Extract the response text
      if (response.choices && response.choices.length > 0 && response.choices[0].message) {
        ğŸ“¤ response.choices[0].message.content
      }
      
      ğŸ“¤ { error: "Unexpected response format" }
    },
    
    // Stream a response from the Azure OpenAI API
    stream: ğŸ”(provider, prompt, callback, options) {
      if (!provider || !provider.api_key) {
        ğŸ“¤ { error: "Provider not initialized or missing API key" }
      }
      
      // Prepare request headers
      ğŸ§  headers = {
        "api-key": provider.api_key,
        "Content-Type": "application/json"
      }
      
      // Prepare request data
      ğŸ§  data = {
        messages: [
          { role: "user", content: prompt }
        ],
        temperature: options?.temperature !== undefined ? options.temperature : provider.temperature,
        max_tokens: options?.max_tokens || provider.max_tokens,
        stream: true
      }
      
      // Add system message if provided
      if (options?.system_message) {
        data.messages.unshift({ role: "system", content: options.system_message })
      }
      
      // Construct the URL for the chat completions endpoint
      ğŸ§  url = provider.endpoint + "/openai/deployments/" + provider.deployment_name + "/chat/completions?api-version=" + provider.api_version
      
      // In a real implementation, we would use a streaming approach
      // For now, we'll simulate it with a non-streaming request
      
      // Make the API request (non-streaming for now)
      ğŸ§  response = http.post(url, data, headers)
      
      // Check for errors
      if (response.error) {
        ğŸ“¤ { error: "API request failed: " + response.error }
      }
      
      // Extract the response text and call the callback
      if (response.choices && response.choices.length > 0 && response.choices[0].message) {
        callback(response.choices[0].message.content)
        ğŸ“¤ true
      }
      
      ğŸ“¤ { error: "Unexpected response format" }
    },
    
    // Estimate the number of tokens in the text
    get_token_count: ğŸ”(provider, text) {
      // Simple estimation: ~4 characters per token for English text
      ğŸ“¤ Math.ceil(text.length / 4)
    },
    
    // Get the capabilities of this provider
    get_capabilities: ğŸ”(provider) {
      // Capabilities depend on the specific deployment
      // For now, we'll use conservative defaults
      ğŸ§  capabilities = {
        supports_streaming: true,
        supports_function_calling: true,
        supports_vision: false,  // Depends on the model deployed
        max_context_length: 4096  // Default
      }
      
      ğŸ“¤ capabilities
    }
  }
  
  // Merge the base interface with our implementation
  for (ğŸ§  key in base) {
    if (!azure_provider[key]) {
      azure_provider[key] = base[key]
    }
  }
  
  ğŸ“¤ azure_provider
}

// Export the Azure OpenAI provider creator function
ğŸ“¤ {
  create_azure_provider: create_azure_provider
}
